{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "> API details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "from celverapi.core import *\n",
    "import pandas as pd\n",
    "import concurrent.futures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "\n",
    "# ---- Loading DataFrame\n",
    "\n",
    "df = pd.read_parquet('C:/Users/kevinkeipert/Desktop/Data/example.parquet')\n",
    "\n",
    "# ---- Project Variables \n",
    "\n",
    "id_column = \"store_nbr\"         #Name of the Column where the Product IDs are located.\n",
    "                                #This will split the DataFrame into smaller DataFrames from every single unique value in the column.\n",
    "\n",
    "date_col = \"date\"               #Name of the Colum where the Date is located. \n",
    "\n",
    "date_to_split = \"2017-08-01\"    #YYYY-MM-DD from where we want to split the DataFrames into Training and Test sets. \n",
    "                                #In this example, the Train set will be every day until 2017-08-01. And the Test set from 2017-08-01 (inclusive), until the end of the DataFrame.\n",
    "\n",
    "value_col = \"sales\"             #Name of the Column where the values to messure are located. \n",
    "                                #In this example is the sales Column that contains the amount of money per day. \n",
    "\n",
    "forecast_horizon = 15           #Amount of steps to forecast "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "\n",
    "def unique_dataframe_values(id_column: str):\n",
    "    \n",
    "    \"\"\"\"\n",
    "    This Function returns a List of DataFrames from every unique Value in the ID Column.\n",
    "\n",
    "    -   id_column (String) : Name of the Column from the DataFrame where the Unique IDs are located. This will split the DataFrame into smaller DataFrames from every single unique value in the column.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    list_of_df = []\n",
    "\n",
    "    array_uvalues = df[id_column].unique()\n",
    "\n",
    "    for value in array_uvalues: \n",
    "\n",
    "        df_value = df[df[id_column] == value]\n",
    "        list_of_df.append(df_value)\n",
    "\n",
    "    return list_of_df \n",
    "\n",
    "list_of_df = unique_dataframe_values(id_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# ---- Splitting the DataFrames into Train and Test  ----\n",
    "# ---- The DataFrames are saved in 2 Differents Lists, one for TRAIN DF and other for TEST DF ----\n",
    "\n",
    "def split_into_train_test_dataframe(date_col: str, date_to_split: str):\n",
    "\n",
    "    \"\"\"\n",
    "    The function splits the DataFrames into a Traing and Test sets.\n",
    "    The DataFrames are saved in 2 Differents Lists, one for TRAIN DF and other for TEST DF.\n",
    "    It returs two lists: list_of_trained_df, list_of_test_df\n",
    "\n",
    "    -   list_of_df (List) : Lists of DataFrames with only one ID.\n",
    "    -   date_col (String) : Name of the Colum where the Date is located.\n",
    "    -   date_to_split (String) : YYYY-MM-DD from where we want to split the DataFrames into Training and Test sets. Example, the Train set will be every day until 2017-08-01. And the Test set from 2017-08-01 (inclusive), until the end of the DataFrame.\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    list_df_train = []\n",
    "    list_df_val = []\n",
    "    \n",
    "    for df in list_of_df:\n",
    "    \n",
    "        df_train = df[df[date_col] < date_to_split]\n",
    "        df_val = df[df[date_col] >= date_to_split]\n",
    "\n",
    "        list_df_train.append(df_train)\n",
    "        list_df_val.append(df_val)\n",
    "\n",
    "    return list_df_train, list_df_val\n",
    "\n",
    "list_df_train, list_df_val = split_into_train_test_dataframe(date_col, date_to_split)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# ---- Creating TimeSeries Object from the respective DataFrames ---------\n",
    "# ---- We append the objects again in two different lists ----------------\n",
    "\n",
    "\n",
    "def converting_dataframes_into_objects(date_col, id_col, value_col, forecast_horizon: int):\n",
    "\n",
    "    trained_ts = []\n",
    "    valed_ts = []\n",
    "\n",
    "    for df in list_df_train:\n",
    "\n",
    "        ts_train = TimeSeries(df, date_col, id_col, value_col)\n",
    "        trained_ts.append(ts_train)\n",
    "\n",
    "    for df in list_df_val:\n",
    "\n",
    "        ts_val = TimeSeries(df, date_col, id_col, value_col)\n",
    "        valed_ts.append(ts_val)\n",
    "\n",
    "    dict_of_forecasting_tasks = {}  \n",
    "\n",
    "    for i in range(len(trained_ts)):\n",
    "\n",
    "        ft = ForecastingTask(train=trained_ts[i], out_of_sample=None, validation=valed_ts[i], forecast_horizon= forecast_horizon)\n",
    "        dict_of_forecasting_tasks[trained_ts[i].get_id()] = ft\n",
    "\n",
    "    \n",
    "    return trained_ts, valed_ts, dict_of_forecasting_tasks\n",
    "\n",
    "trained_ts, valed_ts, dict_of_forecasting_tasks = converting_dataframes_into_objects(date_col, id_column, value_col, forecast_horizon)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "# ---- Functions from the respective Models ---------    \n",
    "\n",
    "def ets_func(ft: ForecastingTask):\n",
    "    \n",
    "    ets = ETSModel(ft)\n",
    "    ets.fit()\n",
    "    prediction = ets.predict()\n",
    "    id = ft.train.get_id()\n",
    "    datum = ft.validation.get_datum()\n",
    "\n",
    "    final_dataframe = prediction.to_frame(name='Qty')\n",
    "    final_dataframe['ID'] = id \n",
    "    final_dataframe['Datum'] = datum\n",
    "    final_dataframe['Model'] = 'ETS-Model'\n",
    "    \n",
    "    return final_dataframe\n",
    "\n",
    "def naive_func(ft: ForecastingTask):\n",
    "\n",
    "    naive = NaiveForecast(ft)\n",
    "    naive.fit()\n",
    "    prediction = naive.predict()\n",
    "    id = ft.train.get_id()\n",
    "    datum = ft.validation.get_datum()\n",
    "\n",
    "    final_dataframe = prediction.to_frame(name='Qty')\n",
    "    final_dataframe['ID'] = id \n",
    "    final_dataframe['Datum'] = datum\n",
    "    final_dataframe['Model'] = 'Naive Model'\n",
    "    \n",
    "    return final_dataframe\n",
    "\n",
    "def trend_func(ft: ForecastingTask):\n",
    "\n",
    "    theta = TrendForecast(ft)\n",
    "    theta.fit()\n",
    "    prediction = theta.predict()\n",
    "    id = ft.train.get_id()\n",
    "    datum = ft.validation.get_datum()\n",
    "\n",
    "    final_dataframe = prediction.to_frame(name='Qty')\n",
    "    final_dataframe['ID'] = id \n",
    "    final_dataframe['Datum'] = datum\n",
    "    final_dataframe['Model'] = 'Trend Model'\n",
    "    \n",
    "    return final_dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def main():\n",
    "    with concurrent.futures.ProcessPoolExecutor() as executor: \n",
    "\n",
    "        # Run, for example various TimeSeries simultainly for Arima Model \n",
    "        ets_prediction = executor.map(ets_func, dict_of_forecasting_tasks.values())\n",
    "        naive_prediction = executor.map(naive_func, dict_of_forecasting_tasks.values())\n",
    "        theta_prediction = executor.map(trend_func, dict_of_forecasting_tasks.values())\n",
    "\n",
    "        # List with all the Forecasted Objects \n",
    "        final_results_ets = []\n",
    "        final_results_naive = []\n",
    "        final_results_trend = []\n",
    "        \n",
    "        #Appending the objects to the list\n",
    "        for result in ets_prediction:\n",
    "            final_results_ets.append(result)\n",
    "        \n",
    "        for result in naive_prediction:\n",
    "            final_results_naive.append(result)\n",
    "\n",
    "        for result in theta_prediction:\n",
    "            final_results_trend.append(result)\n",
    "\n",
    "        result_ets = pd.concat(final_results_ets)\n",
    "        result_naive = pd.concat(final_results_naive)\n",
    "        result_trend = pd.concat(final_results_trend)\n",
    "\n",
    "        print(result_ets)\n",
    "        print(result_naive)\n",
    "        print(result_trend)\n",
    "\n",
    "if __name__ == '__main__':  \n",
    "     main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
